{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3fd5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "# For presentations purposes only (not needed in Colab)\n",
    "#plt.style.use('notebook.mplstyle')\n",
    "# Keeps the kernel from dying in notebooks on Windows machines (not needed in Colab)\n",
    "#import os\n",
    "#os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "# Define my own colormap\n",
    "colors = plt.get_cmap(\"tab10\")\n",
    "gray = 0.5\n",
    "red = np.hstack([np.linspace(colors(0)[0], gray, 128), np.linspace(gray, colors(1)[0], 127)])\n",
    "green = np.hstack([np.linspace(colors(0)[1], gray, 128), np.linspace(gray, colors(1)[1], 127)])\n",
    "blue = np.hstack([np.linspace(colors(0)[2], gray, 128), np.linspace(gray, colors(1)[2], 127)])\n",
    "rgb = np.vstack([red, green, blue]).T\n",
    "my_cmap = ListedColormap(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de67243",
   "metadata": {},
   "source": [
    "### Generate fake data to work with\n",
    "We assume that the data lies on a 1-D manifold in a 2-D input space. In simpler terms, this means that we assume that the data can be approximated with a curve in the 2-D input space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f970cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 2-D input data with quadratic relatinship\n",
    "n_data_points = 7\n",
    "x_lim = [-1, 1]\n",
    "x1 = np.linspace(x_lim[0], x_lim[1], n_data_points)\n",
    "x2 = -x1**2 + 0.15*np.random.randn(x1.size)\n",
    "X = np.vstack([x1, x2]).T\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(X[:, 0], X[:, 1], 'o', alpha=0.5)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dba18c1",
   "metadata": {},
   "source": [
    "### Define an autoencoder in PyTorch\n",
    "We define a simple autoencoder with two feed-forward neural networks: one encoder network and one decoder network. We make both networks \"symmetric\" with a single hidden layer containing equally many hidden neurons. There is no specific reason for why the encoder and decoder would have to be symmetric, but people tend to use this as a defualt starting pont due to historical reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3230f64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch AE class\n",
    "class AE(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "            \n",
    "        # Encoder\n",
    "        # Linear layers followed by \n",
    "        # sigmoid activation functions\n",
    "        # 2 ==> n_hidden_neurons  ==> 1\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(2, self.n_hidden_neurons),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(self.n_hidden_neurons, 1)\n",
    "        )\n",
    "          \n",
    "        # Decoder\n",
    "        # Linear layers followed by \n",
    "        # sigmoid activation functions\n",
    "        # 1 ==> n_hidden_neurons ==> 2\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1, self.n_hidden_neurons),\n",
    "            torch.nn.Sigmoid(),\n",
    "            torch.nn.Linear(self.n_hidden_neurons, 2),\n",
    "        )\n",
    "  \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91002721",
   "metadata": {},
   "source": [
    "### Train the network\n",
    "1. In order to train the network we need to attach our data to a dataloader. The batch size determines how many data points we use to estimate the gradient before updating the model's parameters.\n",
    "2. We use the mean squarred error (MSE) as our loss function as this is equivalen to minimizing the reconstruciton error between inputs and outputs.\n",
    "3. The optimizer takes care of updating the model's parameters. The simplest optimizer is stochastic gradient descent (SGD) which is essentially the same as normal gradient descent but with the gradient estimated from mini batches (the batch size) isntead of all available data. However, learnign can often be accelerated using additional tricks (momentum and adaptive learning rates), and the easiest way  to include these are to use the Adam optimizer instead of SGD.\n",
    "4. The learning rate you will have to set your self based on trial and error.\n",
    "5. Training duration is normally measured in \"Epochs\", where one epoch corresponds to using all training data once to update the model parameters. For example: if your batch size is 16 and you have 64 data points, then one epoch corresponds to 4 batches or parameter updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the data to a DataLoader \n",
    "# PyTorch wants all values as floats thus the conversion below.\n",
    "loader = torch.utils.data.DataLoader(dataset = torch.from_numpy(X).float(),\n",
    "                                     batch_size = 16,\n",
    "                                     shuffle = True)\n",
    "\n",
    "# Use MSE as the loss function\n",
    "loss_function = torch.nn.MSELoss()\n",
    "\n",
    "# Model initialization\n",
    "n_hidden_neurons = 50\n",
    "model = AE(n_hidden_neurons)\n",
    "\n",
    "# Using the Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Train the network\n",
    "epochs = 1500\n",
    "mse_values = []\n",
    "for epoch in range(epochs):\n",
    "    for x_batch in loader:\n",
    "        \n",
    "        # Autoencoder output\n",
    "        reconstructed = model(x_batch)\n",
    "\n",
    "        # Calculating the loss function\n",
    "        loss = loss_function(reconstructed, x_batch)\n",
    "\n",
    "        # Set old gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Computes the gradient\n",
    "        loss.backward()\n",
    "        # Perform the parameter update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Storing the MSE values in a list for plotting\n",
    "        mse_values.append(loss.detach().numpy())\n",
    "\n",
    "# Visualize training progress\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(mse_values)\n",
    "ax.set(xlabel='Iterations', ylabel='Loss (MSE)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eeeef63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send our data through the trained model\n",
    "# The from numpy().float() -> detach().numpy() is needed as we are\n",
    "# first convert our numpy array to a torch tensor and then back to a numpy array\n",
    "X_hat = model(torch.from_numpy(X).float()).detach().numpy()\n",
    "# We can also just run the encoder network to see where each data point lives in \n",
    "# 1-D latent space.\n",
    "latent_var = model.encoder(torch.from_numpy(X).float()).detach().numpy()\n",
    "# Interpolate in the latent space\n",
    "latent_var_interp = np.linspace(latent_var.min(), latent_var.max(), 100)\n",
    "X_hat_interp = model.decoder(torch.from_numpy(latent_var_interp[:, np.newaxis]).float()).detach().numpy()\n",
    "\n",
    "# Visualize input -> latent space -> output conversion\n",
    "# We color code each data point based on where it lives in the latent\n",
    "# space so as to be able to follow how single data points pass through the autoencoder.\n",
    "marker_size = 100\n",
    "c_map = cm.viridis\n",
    "fig, axs = plt.subplots(1, 3, figsize=[15, 4])\n",
    "\n",
    "# Input\n",
    "axs[0].scatter(X[:, 0], X[:, 1], marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[0].set(xticks=[], yticks=[], xlabel='$x_1$', ylabel='$x_2$', title='Input')\n",
    "\n",
    "# Latent space\n",
    "axs[1].plot(latent_var_interp, np.zeros(latent_var_interp.size), 'k-')\n",
    "axs[1].scatter(latent_var, np.zeros(latent_var.size), marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[1].set(xticks=[], yticks=[], xlabel='Latent variable', title='Laten space')\n",
    "\n",
    "# Output\n",
    "axs[2].plot(np.stack([X[:, 0], X_hat[:, 0]]), np.stack([X[:, 1], X_hat[:, 1]]), 'k:')\n",
    "axs[2].scatter(X[:, 0], X[:, 1], marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[2].plot(X_hat_interp[:, 0], X_hat_interp[:, 1], 'k-')\n",
    "#axs[2].scatter(X_hat[:, 0], X_hat[:, 1], marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[2].set(xticks=[], yticks=[], xlabel='$x_1$', ylabel='$x_2$', title='Output');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e64a4b6",
   "metadata": {},
   "source": [
    "### Define a variational autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b534ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a PyTorch variational AE class\n",
    "class VariationalEncoder(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.N = torch.distributions.Normal(0, 1)\n",
    "        self.linear1 = torch.nn.Linear(2, self.n_hidden_neurons)\n",
    "        self.linear2 = torch.nn.Linear(self.n_hidden_neurons, 1)\n",
    "        self.linear3 = torch.nn.Linear(self.n_hidden_neurons, 1)\n",
    "        self.kl = 0       \n",
    "  \n",
    "    # Encoder\n",
    "    # Linear layers followed by \n",
    "    # sigmoid activation functions\n",
    "    # 2 ==> n_hidden_neurons  ==> 1 (one value for mu and one log_sigma)\n",
    "    def forward(self, x):\n",
    "        # Network that output mu and log_sigma\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        mu = self.linear2(x)\n",
    "        log_sigma = self.linear3(x)\n",
    "        # Sample from a normal distribution\n",
    "        z = mu + torch.exp(log_sigma)*self.N.sample(mu.shape)\n",
    "        # Compute the KL divergence\n",
    "        self.kl = (torch.exp(log_sigma)**2 + mu**2 - log_sigma - 1/2).sum()\n",
    "        return z, mu, log_sigma\n",
    "\n",
    "class VariationalDecoder(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super().__init__()\n",
    "          \n",
    "        self.n_hidden_neurons = n_hidden_neurons\n",
    "        self.linear1 = torch.nn.Linear(1, self.n_hidden_neurons)\n",
    "        self.linear2 = torch.nn.Linear(self.n_hidden_neurons, 2)          \n",
    "\n",
    "    # Decoder\n",
    "    # Linear layers followed by \n",
    "    # sigmoid activation functions\n",
    "    # 1 ==> n_hidden_neurons ==> 2\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "class VariationalAutoencoder(torch.nn.Module):\n",
    "    def __init__(self, n_hidden_neurons):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(n_hidden_neurons)\n",
    "        self.decoder = VariationalDecoder(n_hidden_neurons)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)[0]\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0223f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "alpha = 0.02\n",
    "n_hidden_neurons = 50\n",
    "model = VariationalAutoencoder(n_hidden_neurons)\n",
    "\n",
    "# Using the Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "# Train the network\n",
    "epochs = 1500\n",
    "mse_values = []\n",
    "for epoch in range(epochs):\n",
    "    for x_batch in loader:\n",
    "        \n",
    "        # Autoencoder output\n",
    "        reconstructed = model(x_batch)\n",
    "\n",
    "        # Calculating the loss function: MSE + KL divergence\n",
    "        mse = ((x_batch - reconstructed)**2).sum()\n",
    "        loss = mse + alpha*model.encoder.kl\n",
    "\n",
    "        # Set old gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "        # Computes the gradient\n",
    "        loss.backward()\n",
    "        # Perform the parameter update\n",
    "        optimizer.step()\n",
    "\n",
    "        # Storing the MSE values in a list for plotting\n",
    "        mse_values.append(loss.detach().numpy())\n",
    "\n",
    "# Visualize training progress\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(mse_values)\n",
    "ax.set(xlabel='Iterations', ylabel='Loss (MSE)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05364e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send our data through the encoder\n",
    "z, z_mu, log_sigma = model.encoder(torch.from_numpy(X).float())\n",
    "# Send the z_mu values through the decoder\n",
    "X_hat = model.decoder(z_mu).detach().numpy()\n",
    "\n",
    "# Detach tensors and convert back to numpy arrays\n",
    "z = z.detach().numpy()\n",
    "z_mu = z_mu.detach().numpy()\n",
    "sigma = np.exp(log_sigma.detach().numpy())\n",
    "latent_var_interp = np.linspace(z_mu.min(), z_mu.max(), 200)\n",
    "X_hat_interp = model.decoder(torch.from_numpy(latent_var_interp[:, np.newaxis]).float()).detach().numpy()\n",
    "\n",
    "# Visualize input -> latent space -> output conversion\n",
    "# We color code each data point based on where it lives in the latent\n",
    "# space so as to be able to follow how single data points pass through the autoencoder.\n",
    "fig, axs = plt.subplots(1, 3, figsize=[15, 4])\n",
    "\n",
    "# Input\n",
    "axs[0].scatter(X[:, 0], X[:, 1], marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[0].set(xticks=[], yticks=[], xlabel='$x_1$', ylabel='$x_2$', title='Input')\n",
    "\n",
    "# Latent space\n",
    "for i in range(X.shape[0]):\n",
    "    axs[1].plot(latent_var_interp, np.exp(-(latent_var_interp-z_mu[i])**2 / 2 / sigma[i]**2), '-', color='gray')\n",
    "axs[1].plot(latent_var_interp, np.zeros(latent_var_interp.size), 'k-')\n",
    "axs[1].scatter(z_mu, np.zeros(z_mu.size), marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[1].set(xticks=[], yticks=[], xlabel='Latent variable, z', title='Laten space')\n",
    "\n",
    "# Output\n",
    "axs[2].plot(np.stack([X[:, 0], X_hat[:, 0]]), np.stack([X[:, 1], X_hat[:, 1]]), 'k:')\n",
    "axs[2].scatter(X[:, 0], X[:, 1], marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[2].plot(X_hat_interp[:, 0], X_hat_interp[:, 1], 'k-')\n",
    "#axs[2].scatter(X_hat[:, 0], X_hat[:, 1], marker_size, latent_var, cmap=my_cmap, alpha=0.5)\n",
    "axs[2].set(xticks=[], yticks=[], xlabel='$x_1$', ylabel='$x_2$', title='Output');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
