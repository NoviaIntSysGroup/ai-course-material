{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8852c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.model_selection import KFold\n",
    "# Only use this if running the notebook on your local machine\n",
    "#plt.style.use('notebook.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2773e145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_points = 11\n",
    "x_lim = [-1.5, 1.5]\n",
    "# parameters for the data that we generate: y = w0x + w1x^2 + w2x^3 + ... w9x^10 + b\n",
    "w = np.array([1, 1, 1, 0, 0, 0, 0, 0, 0, 0]).reshape(10, 1)\n",
    "powers = np.arange(1, w.size+1)\n",
    "\n",
    "# Generate linearly spaced data points\n",
    "x_data = np.linspace(x_lim[0], x_lim[1], n_points)\n",
    "# Compute the powers of x\n",
    "X_train = np.stack([x_data**i for i in powers]).T\n",
    "# @ denotes matrix multiplication\n",
    "y_train = X_train @ w\n",
    "# Add normally distributed noise\n",
    "y_train = y_train + np.random.randn(n_points, 1)\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x_data, y_train, 'o', alpha=0.5)\n",
    "ax.set(xlabel='x', ylabel='y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3aa356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an array of linearly spaced dense x values to show\n",
    "# model prediction between the original data points (interpolation)\n",
    "x_dense = np.linspace(x_lim[0], x_lim[1], 101)\n",
    "X_dense = np.stack([x_dense**i for i in powers]).T\n",
    "\n",
    "# Fit a 10th degree polynomial using linear regression\n",
    "lin_reg = LinearRegression(fit_intercept=True)\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_hat = lin_reg.predict(X_dense)\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x_data, y_train, 'o', alpha=0.5, label='Data')\n",
    "ax.plot(x_dense, y_hat, 'k-', label='Model')\n",
    "ax.set(xlabel='x', ylabel='y');\n",
    "ax.legend()\n",
    "\n",
    "# Print the fitted 10th degree polynomial function\n",
    "polynom = ['+ {:1.3g}x^{:g}'.format(lin_reg.coef_[0, i], i+1) for i in range(10)]\n",
    "polynom = '{:1.3g} '.format(lin_reg.intercept_[0]) +  ' '.join(polynom)\n",
    "print(polynom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bdda5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define alpha (regularization) values to try\n",
    "alpha_vals = np.logspace(-4, 3, 8)\n",
    "\n",
    "# Create a figure window\n",
    "fig = plt.figure(figsize=[17, 6])\n",
    "\n",
    "# Loop over all alpha values\n",
    "for i in range(alpha_vals.size):\n",
    "    # Create a subplot\n",
    "    ax = fig.add_subplot(2, 4, i+1)\n",
    "    \n",
    "    # Ridge regression\n",
    "    ridge_reg = Ridge(fit_intercept=True, alpha=alpha_vals[i], max_iter=1e6)\n",
    "    ridge_reg.fit(X_train, y_train)\n",
    "    y_hat = ridge_reg.predict(X_dense)\n",
    "    \n",
    "    # Plot the fitted model\n",
    "    ax.plot(x_data, y_train, 'o', alpha=0.5)\n",
    "    ax.plot(x_dense, y_hat, 'k-', label='alpha={:1.5g}'.format(alpha_vals[i]))\n",
    "    ax.set(xticks=[], yticks=[]);\n",
    "    ax.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdbbb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross-validation\n",
    "n_folds = 5\n",
    "kf = KFold(n_splits=n_folds, shuffle=True, random_state=None)\n",
    "\n",
    "fig = plt.figure(figsize=[17, 4])\n",
    "\n",
    "# Loop over all splits\n",
    "split_id = 1\n",
    "for train_index, val_index in kf.split(X_train):\n",
    "    # Create a subplot\n",
    "    ax = fig.add_subplot(1, n_folds, split_id)\n",
    "    # Plot training and validation data points with separate colors\n",
    "    ax.plot(x_data[train_index], y_train[train_index], 'o', label='Train.')\n",
    "    ax.plot(x_data[val_index], y_train[val_index], 'o', label='Val.')\n",
    "    ax.set(xlabel='x', ylabel='y', xticks=[], yticks=[]);\n",
    "    ax.legend()\n",
    "    split_id += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03190f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_per_alpha = []\n",
    "# Loop over all alpha values\n",
    "for i in range(alpha_vals.size):\n",
    "    \n",
    "    ridge_reg = Ridge(fit_intercept=True, alpha=alpha_vals[i], max_iter=1e6)\n",
    "    \n",
    "    mse_tmp = 0\n",
    "    # Loop over all division into training and validation sets\n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_cv_train, X_cv_val = X_train[train_index, :], X_train[val_index, :]\n",
    "        y_cv_train, y_cv_val = y_train[train_index, :], y_train[val_index, :]\n",
    "        # Fit a model and compute the MSE for validation set predictions\n",
    "        ridge_reg.fit(X_cv_train, y_cv_train)\n",
    "        y_hat_cv_val = ridge_reg.predict(X_cv_val)\n",
    "        mse_tmp += np.sum((y_cv_val - y_hat_cv_val)**2)\n",
    "    # Store away the final MSE value for each alpha value\n",
    "    mse_per_alpha.append(mse_tmp)\n",
    "    \n",
    "# Plot the found MSE value for each alpha value\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(alpha_vals, mse_per_alpha, 'ko-')\n",
    "ax.set(xlabel='alpha', ylabel='MSE', xscale='log', yscale='log');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38384761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a ridge regression model with built in cross-validation\n",
    "ridge_reg_cv = RidgeCV(fit_intercept=True, alphas=alpha_vals, cv=None)\n",
    "ridge_reg_cv.fit(X_train, y_train)\n",
    "y_hat = ridge_reg_cv.predict(X_dense)\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(x_data, y_train, 'o', alpha=0.5, label='Data')\n",
    "ax.plot(x_dense, y_hat, 'k-', label='alpha={:1.4g}'.format(ridge_reg_cv.alpha_))\n",
    "ax.set(xlabel='x', ylabel='y');\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744b4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
