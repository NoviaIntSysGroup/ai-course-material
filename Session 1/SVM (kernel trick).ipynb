{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6f9d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap\n",
    "# Only use this if running the notebook on your local machine\n",
    "#plt.style.use('notebook.mplstyle')\n",
    "colors = plt.get_cmap(\"tab10\")\n",
    "# Define my own colormap\n",
    "gray = 0.75\n",
    "red = np.hstack([np.linspace(colors(0)[0], gray, 128), np.linspace(gray, colors(1)[0], 127)])\n",
    "green = np.hstack([np.linspace(colors(0)[1], gray, 128), np.linspace(gray, colors(1)[1], 127)])\n",
    "blue = np.hstack([np.linspace(colors(0)[2], gray, 128), np.linspace(gray, colors(1)[2], 127)])\n",
    "rgb = np.vstack([red, green, blue]).T\n",
    "my_cmap = ListedColormap(rgb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd585d18",
   "metadata": {},
   "source": [
    "### Kernel trick\n",
    "The algorithm for finding the optimal hyperplane can be formulated in such a way that the data points ($\\mathbf{x}_i$) only enter the equations as dot products on the form: $\\mathbf{x}_i^T\\mathbf{x}_j = x_1^ix_1^j + x_2^ix_2^j$ (assuming two dimensional data). Kernels correspond to computations that you can do directly on the data points ($\\mathbf{x}_i$ and $\\mathbf{x}_j$) that are mathematically equivalent to a dot product between data points with additional features. For exampel, the quaratic kernel $K(\\mathbf{x}_i, \\mathbf{x}_j) = (\\mathbf{x}_i^T\\mathbf{x}_j + 1)^2$ is equivalent to taking the dot product between the feature vectors $[1, \\sqrt{2}x_1, \\sqrt{2}x_2, \\sqrt{2}x_1x_2, x_1^2, x_2^2]_i^T[1, \\sqrt{2}x_1, \\sqrt{2}x_2, \\sqrt{2}x_1x_2, x_1^2, x_2^2]_j$. That is, this kernel directly computes the dot product between the data points for a case where we would concatenated our original data vectors with quadratic terms. Adding features in this way can be interpreted as projecting our data into a higher dimensional space, and solving the classification problem in that higher dimensional space instead. Classification problems that are not linearly separable often becomes so when projected into a higher dimensional space. This is illustrated in the example below with a quadratic kernel where we summed the three new features into one third for visualization purposes, that is $x_3 = x_1x_2 + x_1^2+x_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f70dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_per_class = 50\n",
    "radius = 6\n",
    "\n",
    "x_class1 =  np.random.randn(n_per_class, 2)\n",
    "thetas = 2*np.pi*np.random.rand(n_per_class)\n",
    "x_class2 = np.stack([radius*np.cos(thetas), radius*np.sin(thetas)]).T\n",
    "x_class2 += np.random.randn(n_per_class, 2)\n",
    "\n",
    "X = np.vstack([x_class1, x_class2])\n",
    "y = np.vstack([np.zeros([n_per_class, 1]), np.ones([n_per_class, 1])])\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(X[y.flatten()==0, 0], X[y.flatten()==0, 1], 'o', alpha=0.75, label='Class 1')\n",
    "ax.plot(X[y.flatten()==1, 0], X[y.flatten()==1, 1], 'o', alpha=0.75, label='Class 2')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90ea08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = X[:, 0]*X[:, 1] + X[:, 0]**2 + X[:, 1]**2\n",
    "X_new = np.hstack([X, x_new[:, np.newaxis]])\n",
    "\n",
    "# Visualize what we have\n",
    "fig = plt.figure(figsize=[8, 6])\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.plot(X_new[y.flatten()==0, 0], X_new[y.flatten()==0, 1], X_new[y.flatten()==0, 2], 'o', alpha=0.5, label='Class 1')\n",
    "ax.plot(X_new[y.flatten()==1, 0], X_new[y.flatten()==1, 1], X_new[y.flatten()==1, 2], 'o', alpha=0.5, label='Class 2')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', zlabel='x_new');\n",
    "ax.legend();\n",
    "ax.view_init(azim=-60, elev=20.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9d5bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1e3, kernel='linear')\n",
    "svc.fit(X_new, y.flatten())\n",
    "\n",
    "x_grid = np.linspace(-radius-3, radius+3, 101)\n",
    "X1, X2 = np.meshgrid(x_grid, x_grid)\n",
    "X3 = (-svc.intercept_ - X1*svc.coef_[0][0] - X2*svc.coef_[0][1])  / svc.coef_[0][2]\n",
    "\n",
    "# Visualize what we have\n",
    "fig = plt.figure(figsize=[8, 6])\n",
    "ax = fig.add_subplot(1, 1, 1, projection='3d')\n",
    "ax.plot(X_new[y.flatten()==0, 0], X_new[y.flatten()==0, 1], X_new[y.flatten()==0, 2], 'o', alpha=0.5, label='Class 1')\n",
    "ax.plot(X_new[y.flatten()==1, 0], X_new[y.flatten()==1, 1], X_new[y.flatten()==1, 2], 'o', alpha=0.5, label='Class 2')\n",
    "ax.plot_surface(X1, X2, X3, color='gray', alpha=0.5)\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', zlabel='x_new');\n",
    "ax.legend();\n",
    "ax.view_init(azim=-60, elev=20.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4cb7d1",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "We can turn classification problems that are not linearly separable to begin with into linearly separable problems by projecting data into a higher dimensional space. The kernel trick is called a \"trick\" because it directly computes the inner product between our data points in that high-dimensional space without ever entering into it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5165f581",
   "metadata": {},
   "source": [
    "### Radial basis function kernel\n",
    "The most common kernel used is the radial basis function (RBF) kernel. It has one hyperparameter called gamma, and the easiest way to graps what it is doing on an intuitive level is to simply try changing the value of it on a simple example (like the one above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a408ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(C=1e2, kernel='rbf', gamma=0.1)\n",
    "svc.fit(X, y.flatten())\n",
    "\n",
    "z_grid = svc.decision_function(np.stack([X1.flatten(), X2.flatten()]).T)\n",
    "y_hat_grid = svc.predict(np.stack([X1.flatten(), X2.flatten()]).T)\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ch = ax.contourf(X1, X2, y_hat_grid.reshape(X1.shape), 2, alpha=0.5, cmap=my_cmap)\n",
    "ax.plot(X[y.flatten()==0, 0], X[y.flatten()==0, 1], 'o', alpha=0.75, label='Class 1')\n",
    "ax.plot(X[y.flatten()==1, 0], X[y.flatten()==1, 1], 'o', alpha=0.75, label='Class 2')\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "ax.legend();\n",
    "cbar = fig.colorbar(ch, label='Predicted class', ticks=[0, 1]);\n",
    "cbar.ax.set_yticklabels([1, 2]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
