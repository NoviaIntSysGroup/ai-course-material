{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df90ad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# Only use this if running the notebook on your local machine\n",
    "#plt.style.use('notebook.mplstyle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4834ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random 2-d data for classification\n",
    "n_per_class = 20\n",
    "mu_class1 = -2\n",
    "mu_class2 = 2\n",
    "x_class1 =  np.random.randn(n_per_class, 2) + mu_class1*np.array([0.5, 1])\n",
    "x_class2 =  np.random.randn(n_per_class, 2) + mu_class2*np.array([0.5, 1])\n",
    "X = np.vstack([x_class1, x_class2])\n",
    "y = np.vstack([np.zeros([n_per_class, 1]), np.ones([n_per_class, 1])])\n",
    "\n",
    "# Include a scaling factor that makes the variation along\n",
    "# one dimension dominate.\n",
    "scale_factor = 10\n",
    "X_scaled = X * np.array([scale_factor, 1])\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(X_scaled[y.flatten()==0, 0], X_scaled[y.flatten()==0, 1], 'o', alpha=0.75, label='Class 1')\n",
    "ax.plot(X_scaled[y.flatten()==1, 0], X_scaled[y.flatten()==1, 1], 'o', alpha=0.75, label='Class 2')\n",
    "ax_lim = scale_factor*np.array([mu_class1-3, mu_class2+2])\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$', xlim=ax_lim, ylim=ax_lim)\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9226e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression(C=1)\n",
    "\n",
    "# Check how the number of iterations required to reach optimal\n",
    "# parameters vary as a function of the scaling asymetry.\n",
    "n_iterations = []\n",
    "scale_factors = np.logspace(0, 4, 21)\n",
    "for scale_factor in scale_factors:\n",
    "    X_scaled_tmp = X * np.array([scale_factor, 1])\n",
    "    log_reg.fit(X_scaled_tmp, y.flatten())\n",
    "    n_iterations.append(log_reg.n_iter_)\n",
    "\n",
    "# Visualize the result\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(scale_factors, n_iterations, 'ko-')\n",
    "ax.set(xscale='log', \n",
    "       xlabel='Scale difference between $x_1$ and $x_2$',\n",
    "       ylabel='Number of iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646ca536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A typical approach is to always rescale your data\n",
    "# so that all features either vary between a fixed range,\n",
    "# usually [-1, 1] or [0, 1], or are normalized. Normalized \n",
    "# usually mean mean-centered (zero mean) and scaled to unit \n",
    "# variance.\n",
    "\n",
    "# Mean-centering and unit-variance scaling is easily accomplished\n",
    "# with a StandardScaler from scikit-learn as\n",
    "scaler = StandardScaler()\n",
    "X_ss = scaler.fit_transform(X_scaled)\n",
    "print(X_ss.mean(axis=0))\n",
    "print(X_ss.std(axis=0))\n",
    "\n",
    "# Visualize what we have\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(X_ss[y.flatten()==0, 0], X_ss[y.flatten()==0, 1], 'o', alpha=0.75, label='Class 1')\n",
    "ax.plot(X_ss[y.flatten()==1, 0], X_ss[y.flatten()==1, 1], 'o', alpha=0.75, label='Class 2')\n",
    "ax_lim = scale_factor*np.array([mu_class1-3, mu_class2+2])\n",
    "ax.set(xlabel='$x_1$', ylabel='$x_2$')\n",
    "ax.set_ylim(ax.get_xlim())\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ddc3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the the comparison above but by rescaling using the standard scaler\n",
    "n_iterations = []\n",
    "scale_factors = np.logspace(0, 4, 21)\n",
    "for scale_factor in scale_factors:\n",
    "    X_scaled = X * np.array([scale_factor, 1])\n",
    "    X_rescaled = scaler.fit_transform(X_scaled)\n",
    "    log_reg.fit(X_rescaled, y.flatten())\n",
    "    n_iterations.append(log_reg.n_iter_)\n",
    "\n",
    "# Visualize the result\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(scale_factors, n_iterations, 'ko-')\n",
    "ax.set(xscale='log', \n",
    "       xlabel='Scale difference between $x_1$ and $x_2$',\n",
    "       ylabel='Number of iterations');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d6419b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
